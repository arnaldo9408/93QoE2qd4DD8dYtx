{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import skvideo.io\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from tensorflow import keras\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import (Flatten, Dense, Activation, MaxPooling2D, Conv2D, InputLayer)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "from numba import double, jit, njit, vectorize\n",
    "\n",
    "import progressbar\n",
    "\n",
    "import time\n",
    "\n",
    "import PIL\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, ConfusionMatrixDisplay)\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import pickle\n",
    "\n",
    "import math   # for mathematical operations\n",
    "\n",
    "import pandas as pd\n",
    "from keras.preprocessing import image   # for preprocessing the images\n",
    "import numpy as np    # for mathematical operations\n",
    "from keras.utils import np_utils\n",
    "from skimage.transform import resize   # for resizing images\n",
    "\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish the path to read all of the images \n",
    "\n",
    "path_training_flip = glob.glob('/Users/arnaldofolder/Documents/Apziva/Fourth Project/images/training/flip/*.jpg')\n",
    "\n",
    "path_training_notflip = glob.glob('/Users/arnaldofolder/Documents/Apziva/Fourth Project/images/training/notflip/*.jpg')\n",
    "\n",
    "path_testing_flip = glob.glob('/Users/arnaldofolder/Documents/Apziva/Fourth Project/images/testing/flip/*.jpg')\n",
    "\n",
    "path_testing_notflip = glob.glob('/Users/arnaldofolder/Documents/Apziva/Fourth Project/images/testing/notflip/*.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will do all the preprocessing for each image to be ready for modeling\n",
    "\n",
    "def image_preprocessing(path):\n",
    "    # Create an empty list to store all the preprocessed images\n",
    "    images = []\n",
    "    # Start by creating a for loop through all the path and make the preprocessing to each image\n",
    "    for i in path:\n",
    "        # Firstly read the image\n",
    "        img = cv2.imread(i)\n",
    "        # Adjust the size so all iamges will have the same size\n",
    "        img = cv2.resize(img, dsize = (70,140), interpolation=cv2.INTER_CUBIC)\n",
    "        # Crop to remove part of the images I don't need for the modeling part\n",
    "        y,h,x,w = 0,100,0,70\n",
    "        img = img[y:y+h, x:x+w]\n",
    "        # Adjust brightness, contrast\n",
    "        alpha=1.5\n",
    "        beta=0.5\n",
    "        img = cv2.addWeighted(img, alpha, np.zeros(img.shape, img.dtype), 0, beta)\n",
    "        # Normalize the images to be black and white by reverting the images and then dividing by 255.0\n",
    "        img = cv2.bitwise_not(img)\n",
    "        img = img/255\n",
    "\n",
    "        # Append the img to the list images\n",
    "        images.append(img)\n",
    "        # Create the video\n",
    "\n",
    "    # Return the list with the preprocessed images\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to create a video from images\n",
    "\n",
    "def video_creator(path, pathIn, time, fps):\n",
    "    frame_video = []\n",
    "    for i in path:\n",
    "        img = cv2.imread(i)\n",
    "        height, width, layers = img.shape\n",
    "        size = (width, height)\n",
    "        frame_video.append(img)\n",
    "    out = cv2.VideoWriter(pathIn, cv2.VideoWriter_fourcc(*'mp4v'), fps, size)\n",
    "    for i in range(len(path)):\n",
    "        out.write(frame_video[i])\n",
    "    \n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and preprocess the training data\n",
    "\n",
    "img_training_flip = image_preprocessing(path = path_training_flip)\n",
    "\n",
    "# Read the training not flip\n",
    "\n",
    "img_training_notflip = image_preprocessing(path = path_training_notflip)\n",
    "\n",
    "# Read the test flip\n",
    "\n",
    "img_testing_flip = image_preprocessing(path = path_testing_flip)\n",
    "\n",
    "# Read the test not flip\n",
    "\n",
    "img_testing_notflip = image_preprocessing(path = path_testing_notflip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels for the problem\n",
    "\n",
    "y_train_flip = [1 for i in range(0, len(img_training_flip))]\n",
    "\n",
    "y_train_notflip = [0 for i in range(0, len(img_training_notflip))]\n",
    "\n",
    "y_test_flip = [1 for i in range(0, len(img_testing_flip))]\n",
    "\n",
    "y_test_notflip = [0 for i in range(0, len(img_testing_notflip))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the video for the training flip\n",
    "\n",
    "video_creator(path = path_training_flip, pathIn = 'training_flip.avi', time = len(path_training_flip), fps = 1)\n",
    "\n",
    "# Create the video for the training not flip\n",
    "\n",
    "video_creator(path = path_training_notflip, pathIn = 'training_notflip.avi', time = len(img_training_notflip), fps = 1)\n",
    "\n",
    "# Create the video for the test flip\n",
    "\n",
    "video_creator(path = path_testing_flip, pathIn = 'test_flip.avi', time = len(img_testing_flip), fps = 1)\n",
    "\n",
    "# Create the video for the test not flip\n",
    "\n",
    "video_creator(path = path_testing_notflip, pathIn = 'test_notflip.avi', time = len(img_testing_notflip), fps = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the X_train, X_test, y_train and y_test for analysis\n",
    "\n",
    "X_train = np.concatenate((img_training_flip, img_training_notflip), axis = 0)\n",
    "\n",
    "X_test = np.concatenate((img_testing_flip, img_testing_notflip), axis = 0)\n",
    "\n",
    "y_train = np.append(y_train_flip, y_train_notflip)\n",
    "\n",
    "y_test = np.append(y_test_flip, y_test_notflip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2392, 100, 70, 3)\n",
      "(2392,)\n",
      "(597, 100, 70, 3)\n",
      "(597,)\n"
     ]
    }
   ],
   "source": [
    "# See if the shapes matches between the X_trian and y_train and the X_test and y_test\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new array that will have the original arrays (labels and values) but they will be shuffled. \n",
    "\n",
    "# Create the array for the train data set\n",
    "\n",
    "X_train_shuffle = []\n",
    "\n",
    "# It is necessary to create a for loop with enumeration as well\n",
    "for i,j in enumerate(X_train):\n",
    "    # The new array would be the array containing the image plus its label \n",
    "    new_array = (j, y_train[i])\n",
    "    # Append the values to the array that will be shuffled\n",
    "    X_train_shuffle.append(new_array)\n",
    "    \n",
    "# Have the new set of arrays\n",
    "X_train_shuffle = np.array(X_train_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the array for the test data set\n",
    "\n",
    "X_test_shuffle = []\n",
    "\n",
    "# It is necessary to create a for loop with enumeration as well\n",
    "for i,j in enumerate(X_test):\n",
    "    # The new array would be the array containing the image plus its label \n",
    "    new_array = (j, y_test[i])\n",
    "    # Append the values to the array that will be shuffled\n",
    "    X_test_shuffle.append(new_array)\n",
    "    \n",
    "# Have the new set of arrays  \n",
    "X_test_shuffle = np.array(X_test_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the random shuffle to make the train and test with no specific order\n",
    "\n",
    "np.random.shuffle(X_train_shuffle)\n",
    "\n",
    "np.random.shuffle(X_test_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate between the X_train and y_train to fit the model\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# Start a for loop into the X_train_shuffle\n",
    "for i in X_train_shuffle:\n",
    "    # The array containing the picture would be the one that is in the index 0\n",
    "    value = i[0]\n",
    "    # The label would be the array that is on the index 1\n",
    "    label = i[1]\n",
    "    # Append the values and the labels to separate arrays\n",
    "    X_train.append(value)\n",
    "    y_train.append(label)\n",
    "\n",
    "# Divide between X_train and y_train to run model\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for the test data set\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "# Start a for loop into the X_test_shuffle\n",
    "for i in X_test_shuffle:\n",
    "    # The array containing the picture would be the one that is in the index 0\n",
    "    value = i[0]\n",
    "    # The label would be the array that is on the index 1\n",
    "    label = i[1]\n",
    "    # Append the values and the labels to separate arrays\n",
    "    X_test.append(value)\n",
    "    y_test.append(label)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2392, 100, 70, 3)\n",
      "(2392,)\n",
      "(597, 100, 70, 3)\n",
      "(597,)\n"
     ]
    }
   ],
   "source": [
    "# Make sure labels are same than the first shapes\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function with the neural networks\n",
    "\n",
    "def neural_network():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation = 'relu', kernel_initializer='he_uniform', \n",
    "                     padding = 'same', input_shape=(100, 70, 3)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "75/75 [==============================] - 14s 189ms/step - loss: 0.3909 - accuracy: 0.8269\n",
      "Epoch 2/15\n",
      "75/75 [==============================] - 12s 157ms/step - loss: 0.1673 - accuracy: 0.9515\n",
      "Epoch 3/15\n",
      "75/75 [==============================] - 10s 135ms/step - loss: 0.1005 - accuracy: 0.9758\n",
      "Epoch 4/15\n",
      "75/75 [==============================] - 10s 135ms/step - loss: 0.0698 - accuracy: 0.9870\n",
      "Epoch 5/15\n",
      "75/75 [==============================] - 10s 137ms/step - loss: 0.0526 - accuracy: 0.9916\n",
      "Epoch 6/15\n",
      "75/75 [==============================] - 11s 150ms/step - loss: 0.0416 - accuracy: 0.9933\n",
      "Epoch 7/15\n",
      "75/75 [==============================] - 13s 169ms/step - loss: 0.0339 - accuracy: 0.9958\n",
      "Epoch 8/15\n",
      "75/75 [==============================] - 11s 149ms/step - loss: 0.0279 - accuracy: 0.9962\n",
      "Epoch 9/15\n",
      "75/75 [==============================] - 12s 164ms/step - loss: 0.0231 - accuracy: 0.9983\n",
      "Epoch 10/15\n",
      "75/75 [==============================] - 13s 169ms/step - loss: 0.0200 - accuracy: 0.9992\n",
      "Epoch 11/15\n",
      "75/75 [==============================] - 10s 127ms/step - loss: 0.0174 - accuracy: 0.9992\n",
      "Epoch 12/15\n",
      "75/75 [==============================] - 10s 131ms/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 13/15\n",
      "75/75 [==============================] - 10s 128ms/step - loss: 0.0139 - accuracy: 0.9996\n",
      "Epoch 14/15\n",
      "75/75 [==============================] - 9s 122ms/step - loss: 0.0124 - accuracy: 0.9996\n",
      "Epoch 15/15\n",
      "75/75 [==============================] - 9s 119ms/step - loss: 0.0107 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14c9c2150>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "\n",
    "model = neural_network()\n",
    "\n",
    "# fit model\n",
    "model.fit(X_train, y_train, epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Get them into 0 and 1 values\n",
    "\n",
    "binary_values = []\n",
    "\n",
    "# Start a for loop to iterate over the predictions array\n",
    "\n",
    "for i in predictions:\n",
    "    if i < 0.5:\n",
    "        binary_values.append(0)\n",
    "    if i >= 0.5:\n",
    "        binary_values.append(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the confusion matrix to evaluate the model\n",
    "\n",
    "cm = confusion_matrix(y_test, binary_values)\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=['not_flip','flip'])\n",
    "cmd.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       307\n",
      "           1       1.00      0.98      0.99       290\n",
      "\n",
      "    accuracy                           0.99       597\n",
      "   macro avg       0.99      0.99      0.99       597\n",
      "weighted avg       0.99      0.99      0.99       597\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the Classification report to get the precision, recall, f1-score\n",
    "\n",
    "print(classification_report(y_test, binary_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### I was able to create a model with 0.99 accuray if a page needs to whether be flipped or not by using deep learning and doing the necessary data preprocessing such as making all the pages the same size, adjusting bright, adding nose, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: flip_page_classifier/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the model using pickle\n",
    "\n",
    "model_classifier = model.save('flip_page_classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   0%|          | 6/2392 [00:00<00:41, 57.43it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video training_video.avi.\n",
      "Moviepy - Writing video training_video.avi\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready training_video.avi\n"
     ]
    }
   ],
   "source": [
    "# Establish the training video by concatenating the flips with the not flips for train and test data sets.\n",
    "\n",
    "#Establish the train video\n",
    "\n",
    "video_1 = VideoFileClip('training_flip.avi')\n",
    "video_2 = VideoFileClip('training_notflip.avi')\n",
    "\n",
    "training_video = concatenate_videoclips([video_1, video_2])\n",
    "\n",
    "training_video.write_videofile('training_video.avi', codec = 'rawvideo')\n",
    "training_video.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   1%|          | 5/597 [00:00<00:12, 47.82it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video test_video.avi.\n",
      "Moviepy - Writing video test_video.avi\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready test_video.avi\n"
     ]
    }
   ],
   "source": [
    "# Establish the test video\n",
    "\n",
    "video_3 = VideoFileClip('test_flip.avi')\n",
    "video_4 = VideoFileClip('test_notflip.avi')\n",
    "\n",
    "test_video= concatenate_videoclips([video_3, video_4])\n",
    "\n",
    "test_video.write_videofile('test_video.avi', codec = 'rawvideo')\n",
    "test_video.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the frames for the training video\n",
    "\n",
    "count = 0\n",
    "\n",
    "videoFile = 'training_video.avi'\n",
    "# Capturing the video from the given path\n",
    "cap = cv2.VideoCapture(videoFile)   \n",
    "# Establish Frame rate\n",
    "frameRate = cap.get(5) \n",
    "\n",
    "x=1\n",
    "\n",
    "filenames_train = []\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    frameId = cap.get(1) #current frame number\n",
    "    ret, frame = cap.read()\n",
    "    if (ret != True):\n",
    "        break\n",
    "    if (frameId % math.floor(frameRate) == 0):\n",
    "        filename =\"frame%d.jpg\" % count;count+=1\n",
    "        filenames_train.append(filename)\n",
    "        cv2.imwrite(filename, frame)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the frames from the test video\n",
    "\n",
    "count = 0\n",
    "\n",
    "videoFile = 'test_video.avi'\n",
    "# Capturing the video from the given path\n",
    "cap = cv2.VideoCapture(videoFile)   \n",
    "# Establish Frame rate\n",
    "frameRate = cap.get(5) \n",
    "\n",
    "x=1\n",
    "\n",
    "filenames_test = []\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    frameId = cap.get(1) #current frame number\n",
    "    ret, frame = cap.read()\n",
    "    if (ret != True):\n",
    "        break\n",
    "    if (frameId % math.floor(frameRate) == 0):\n",
    "        filename =\"frame%d.jpg\" % count;count+=1\n",
    "        filenames_test.append(filename)\n",
    "        cv2.imwrite(filename, frame)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the y_train flip and not flip into the same array\n",
    "\n",
    "y_train = np.append(np.array(y_train_flip), np.array(y_train_notflip))\n",
    "\n",
    "# Append the y_test flip and not flip into the same array\n",
    "\n",
    "y_test = np.append(np.array(y_test_flip), np.array(y_test_notflip))\n",
    "\n",
    "# Create the data frame that will show the frameID and the class for the train data\n",
    "\n",
    "data_train = pd.DataFrame({'frameID': filenames_train, 'flip': y_train})\n",
    "\n",
    "# Create the data frame that will show the frameID and the class for the test data\n",
    "\n",
    "data_test = pd.DataFrame({'frameID': filenames_test, 'flip': y_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly change the data order and reset the index\n",
    "\n",
    "data_train = shuffle(data_train).reset_index(drop = True)\n",
    "\n",
    "data_test = shuffle(data_test).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty array\n",
    "\n",
    "X_train = []   \n",
    "\n",
    "# Loop through the frameID column and store every frame in X\n",
    "for img_name in data_train.frameID:\n",
    "    img = plt.imread('' + img_name)\n",
    "    X_train.append(img)  \n",
    "    \n",
    "# Convert the list to an array\n",
    "X_train = np.array(X_train)    \n",
    "\n",
    "# Define the y_train\n",
    "\n",
    "y_train = data_train['flip'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty array\n",
    "\n",
    "X_test = []   \n",
    "\n",
    "# Loop through the frameID column and store every frame in X\n",
    "for img_name in data_test.frameID:\n",
    "    img = plt.imread('' + img_name)\n",
    "    X_test.append(img)  \n",
    "    \n",
    "# Convert the list to an array\n",
    "\n",
    "X_test = np.array(X_test) \n",
    "\n",
    "# Define the y_test\n",
    "\n",
    "y_test = data_test['flip'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to do the preprocessing for each frame of image of the video\n",
    "\n",
    "def image_preprocessing_frame(data):\n",
    "    # Create an empty list to store all the preprocessed images\n",
    "    images = []\n",
    "    # Start by creating a for loop through all the path and make the preprocessing to each image\n",
    "    for i in data:\n",
    "        # Adjust the size so all iamges will have the same size\n",
    "        img = cv2.resize(i, dsize = (70,140), interpolation=cv2.INTER_CUBIC)\n",
    "        # Crop to remove part of the images I don't need for the modeling part\n",
    "        y,h,x,w = 0,100,0,70\n",
    "        img = img[y:y+h, x:x+w]\n",
    "        # Adjust brightness, contrast\n",
    "        alpha=1.5\n",
    "        beta=0.5\n",
    "        img = cv2.addWeighted(img, alpha, np.zeros(img.shape, img.dtype), 0, beta)\n",
    "        # Append the img to the list images\n",
    "        images.append(img)\n",
    "        # Create the video\n",
    "\n",
    "    # Return the list with the preprocessed images\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the defined function to preprocess the data\n",
    "\n",
    "X_train = image_preprocessing_frame(data = X_train)\n",
    "\n",
    "X_test = image_preprocessing_frame(data = X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include_top=False to remove the top layer and a base model\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(100, 70, 3))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list into arrays\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2392, 3, 2, 512), (597, 3, 2, 512))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will make predictions using this model for X_train and X_valid, get the features, and then use those \n",
    "# features to retrain the model.\n",
    "\n",
    "X_train = base_model.predict(X_train)\n",
    "\n",
    "X_test = base_model.predict(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centering the data\n",
    "\n",
    "X_train = X_train/X_train.max()\n",
    "\n",
    "X_test = X_test/X_test.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network to use to predict if the frame of the video is flip or not flip\n",
    "\n",
    "def model_neural():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation = 'relu', kernel_initializer='he_uniform', \n",
    "                     padding = 'same', input_shape=(3, 2, 512)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.6928 - accuracy: 0.5234 - val_loss: 0.6889 - val_accuracy: 0.6332\n",
      "Epoch 2/100\n",
      "75/75 [==============================] - 1s 7ms/step - loss: 0.6885 - accuracy: 0.6191 - val_loss: 0.6836 - val_accuracy: 0.6750\n",
      "Epoch 3/100\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.6843 - accuracy: 0.6426 - val_loss: 0.6774 - val_accuracy: 0.6985\n",
      "Epoch 4/100\n",
      "75/75 [==============================] - 0s 7ms/step - loss: 0.6793 - accuracy: 0.6672 - val_loss: 0.6700 - val_accuracy: 0.7437\n",
      "Epoch 5/100\n",
      "75/75 [==============================] - 0s 7ms/step - loss: 0.6735 - accuracy: 0.6944 - val_loss: 0.6611 - val_accuracy: 0.7839\n",
      "Epoch 6/100\n",
      "75/75 [==============================] - 1s 7ms/step - loss: 0.6665 - accuracy: 0.7153 - val_loss: 0.6507 - val_accuracy: 0.8090\n",
      "Epoch 7/100\n",
      "75/75 [==============================] - 1s 7ms/step - loss: 0.6585 - accuracy: 0.7283 - val_loss: 0.6391 - val_accuracy: 0.8291\n",
      "Epoch 8/100\n",
      "75/75 [==============================] - 1s 7ms/step - loss: 0.6497 - accuracy: 0.7458 - val_loss: 0.6259 - val_accuracy: 0.8291\n",
      "Epoch 9/100\n",
      "75/75 [==============================] - 1s 7ms/step - loss: 0.6400 - accuracy: 0.7487 - val_loss: 0.6123 - val_accuracy: 0.8325\n",
      "Epoch 10/100\n",
      "75/75 [==============================] - 1s 7ms/step - loss: 0.6292 - accuracy: 0.7579 - val_loss: 0.5959 - val_accuracy: 0.8459\n",
      "Epoch 11/100\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.6174 - accuracy: 0.7680 - val_loss: 0.5801 - val_accuracy: 0.8509\n",
      "Epoch 12/100\n",
      "75/75 [==============================] - 0s 7ms/step - loss: 0.6053 - accuracy: 0.7776 - val_loss: 0.5620 - val_accuracy: 0.8626\n",
      "Epoch 13/100\n",
      "75/75 [==============================] - 0s 7ms/step - loss: 0.5924 - accuracy: 0.7780 - val_loss: 0.5418 - val_accuracy: 0.8777\n",
      "Epoch 14/100\n",
      "75/75 [==============================] - 1s 7ms/step - loss: 0.5788 - accuracy: 0.7843 - val_loss: 0.5234 - val_accuracy: 0.8760\n",
      "Epoch 15/100\n",
      "75/75 [==============================] - 1s 7ms/step - loss: 0.5649 - accuracy: 0.7918 - val_loss: 0.5017 - val_accuracy: 0.8794\n",
      "Epoch 16/100\n",
      "75/75 [==============================] - 0s 7ms/step - loss: 0.5516 - accuracy: 0.7914 - val_loss: 0.4820 - val_accuracy: 0.8928\n",
      "Epoch 17/100\n",
      "75/75 [==============================] - 0s 7ms/step - loss: 0.5388 - accuracy: 0.7972 - val_loss: 0.4657 - val_accuracy: 0.8928\n",
      "Epoch 18/100\n",
      "75/75 [==============================] - 0s 7ms/step - loss: 0.5250 - accuracy: 0.8048 - val_loss: 0.4447 - val_accuracy: 0.8995\n",
      "Epoch 19/100\n",
      "75/75 [==============================] - 1s 7ms/step - loss: 0.5128 - accuracy: 0.8064 - val_loss: 0.4270 - val_accuracy: 0.8995\n",
      "Epoch 20/100\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.5015 - accuracy: 0.8123 - val_loss: 0.4137 - val_accuracy: 0.9028\n",
      "Epoch 21/100\n",
      "75/75 [==============================] - 1s 7ms/step - loss: 0.4909 - accuracy: 0.8127 - val_loss: 0.3948 - val_accuracy: 0.9129\n",
      "Epoch 22/100\n",
      "75/75 [==============================] - 0s 7ms/step - loss: 0.4810 - accuracy: 0.8131 - val_loss: 0.3863 - val_accuracy: 0.9062\n",
      "Epoch 23/100\n",
      "75/75 [==============================] - 0s 7ms/step - loss: 0.4729 - accuracy: 0.8152 - val_loss: 0.3684 - val_accuracy: 0.9196\n",
      "Epoch 24/100\n",
      "75/75 [==============================] - 0s 7ms/step - loss: 0.4642 - accuracy: 0.8173 - val_loss: 0.3585 - val_accuracy: 0.9229\n",
      "Epoch 25/100\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.4568 - accuracy: 0.8177 - val_loss: 0.3461 - val_accuracy: 0.9229\n",
      "Epoch 26/100\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.4500 - accuracy: 0.8236 - val_loss: 0.3386 - val_accuracy: 0.9213\n",
      "Epoch 27/100\n",
      "75/75 [==============================] - 1s 7ms/step - loss: 0.4437 - accuracy: 0.8232 - val_loss: 0.3297 - val_accuracy: 0.9196\n",
      "Epoch 28/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.4384 - accuracy: 0.8282 - val_loss: 0.3189 - val_accuracy: 0.9213\n",
      "Epoch 29/100\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.4331 - accuracy: 0.8290 - val_loss: 0.3138 - val_accuracy: 0.9162\n",
      "Epoch 30/100\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.4274 - accuracy: 0.8324 - val_loss: 0.2997 - val_accuracy: 0.9229\n",
      "Epoch 31/100\n",
      "75/75 [==============================] - 1s 7ms/step - loss: 0.4238 - accuracy: 0.8353 - val_loss: 0.3118 - val_accuracy: 0.9146\n",
      "Epoch 32/100\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.4182 - accuracy: 0.8319 - val_loss: 0.2847 - val_accuracy: 0.9296\n",
      "Epoch 33/100\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.4145 - accuracy: 0.8332 - val_loss: 0.2877 - val_accuracy: 0.9229\n",
      "Epoch 34/100\n",
      "75/75 [==============================] - 1s 7ms/step - loss: 0.4104 - accuracy: 0.8332 - val_loss: 0.2700 - val_accuracy: 0.9380\n",
      "Epoch 35/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.4077 - accuracy: 0.8344 - val_loss: 0.2728 - val_accuracy: 0.9296\n",
      "Epoch 36/100\n",
      "75/75 [==============================] - 0s 7ms/step - loss: 0.4035 - accuracy: 0.8390 - val_loss: 0.2696 - val_accuracy: 0.9280\n",
      "Epoch 37/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.4005 - accuracy: 0.8403 - val_loss: 0.2713 - val_accuracy: 0.9263\n",
      "Epoch 38/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3969 - accuracy: 0.8428 - val_loss: 0.2745 - val_accuracy: 0.9246\n",
      "Epoch 39/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3947 - accuracy: 0.8395 - val_loss: 0.2687 - val_accuracy: 0.9263\n",
      "Epoch 40/100\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.3914 - accuracy: 0.8449 - val_loss: 0.2478 - val_accuracy: 0.9296\n",
      "Epoch 41/100\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.3890 - accuracy: 0.8449 - val_loss: 0.2521 - val_accuracy: 0.9313\n",
      "Epoch 42/100\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.3861 - accuracy: 0.8457 - val_loss: 0.2423 - val_accuracy: 0.9313\n",
      "Epoch 43/100\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.3833 - accuracy: 0.8495 - val_loss: 0.2618 - val_accuracy: 0.9313\n",
      "Epoch 44/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3808 - accuracy: 0.8441 - val_loss: 0.2540 - val_accuracy: 0.9330\n",
      "Epoch 45/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3778 - accuracy: 0.8537 - val_loss: 0.2481 - val_accuracy: 0.9380\n",
      "Epoch 46/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3771 - accuracy: 0.8512 - val_loss: 0.2328 - val_accuracy: 0.9380\n",
      "Epoch 47/100\n",
      "75/75 [==============================] - 1s 7ms/step - loss: 0.3739 - accuracy: 0.8520 - val_loss: 0.2411 - val_accuracy: 0.9397\n",
      "Epoch 48/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3715 - accuracy: 0.8541 - val_loss: 0.2304 - val_accuracy: 0.9430\n",
      "Epoch 49/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3696 - accuracy: 0.8574 - val_loss: 0.2333 - val_accuracy: 0.9414\n",
      "Epoch 50/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3689 - accuracy: 0.8528 - val_loss: 0.2279 - val_accuracy: 0.9430\n",
      "Epoch 51/100\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.3663 - accuracy: 0.8558 - val_loss: 0.2193 - val_accuracy: 0.9464\n",
      "Epoch 52/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3641 - accuracy: 0.8599 - val_loss: 0.2438 - val_accuracy: 0.9380\n",
      "Epoch 53/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3618 - accuracy: 0.8570 - val_loss: 0.2169 - val_accuracy: 0.9464\n",
      "Epoch 54/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3603 - accuracy: 0.8608 - val_loss: 0.2258 - val_accuracy: 0.9430\n",
      "Epoch 55/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3586 - accuracy: 0.8604 - val_loss: 0.2291 - val_accuracy: 0.9447\n",
      "Epoch 56/100\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.3589 - accuracy: 0.8570 - val_loss: 0.2053 - val_accuracy: 0.9564\n",
      "Epoch 57/100\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.3576 - accuracy: 0.8574 - val_loss: 0.2241 - val_accuracy: 0.9447\n",
      "Epoch 58/100\n",
      "75/75 [==============================] - 1s 7ms/step - loss: 0.3545 - accuracy: 0.8637 - val_loss: 0.2259 - val_accuracy: 0.9481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "75/75 [==============================] - 1s 7ms/step - loss: 0.3529 - accuracy: 0.8616 - val_loss: 0.2041 - val_accuracy: 0.9514\n",
      "Epoch 60/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3503 - accuracy: 0.8620 - val_loss: 0.2007 - val_accuracy: 0.9581\n",
      "Epoch 61/100\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.3496 - accuracy: 0.8633 - val_loss: 0.2155 - val_accuracy: 0.9447\n",
      "Epoch 62/100\n",
      "75/75 [==============================] - 0s 7ms/step - loss: 0.3488 - accuracy: 0.8637 - val_loss: 0.1993 - val_accuracy: 0.9581\n",
      "Epoch 63/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3474 - accuracy: 0.8629 - val_loss: 0.2040 - val_accuracy: 0.9514\n",
      "Epoch 64/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3457 - accuracy: 0.8633 - val_loss: 0.2060 - val_accuracy: 0.9447\n",
      "Epoch 65/100\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.3430 - accuracy: 0.8633 - val_loss: 0.1786 - val_accuracy: 0.9732\n",
      "Epoch 66/100\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.3485 - accuracy: 0.8608 - val_loss: 0.1849 - val_accuracy: 0.9715\n",
      "Epoch 67/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3420 - accuracy: 0.8679 - val_loss: 0.1959 - val_accuracy: 0.9548\n",
      "Epoch 68/100\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.3402 - accuracy: 0.8675 - val_loss: 0.1990 - val_accuracy: 0.9548\n",
      "Epoch 69/100\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.3395 - accuracy: 0.8671 - val_loss: 0.1980 - val_accuracy: 0.9548\n",
      "Epoch 70/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3387 - accuracy: 0.8666 - val_loss: 0.2139 - val_accuracy: 0.9497\n",
      "Epoch 71/100\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.3388 - accuracy: 0.8637 - val_loss: 0.2194 - val_accuracy: 0.9481\n",
      "Epoch 72/100\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.3365 - accuracy: 0.8666 - val_loss: 0.2175 - val_accuracy: 0.9464\n",
      "Epoch 73/100\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.3353 - accuracy: 0.8675 - val_loss: 0.1786 - val_accuracy: 0.9715\n",
      "Epoch 74/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3343 - accuracy: 0.8650 - val_loss: 0.2047 - val_accuracy: 0.9497\n",
      "Epoch 75/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3324 - accuracy: 0.8675 - val_loss: 0.1811 - val_accuracy: 0.9732\n",
      "Epoch 76/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3313 - accuracy: 0.8704 - val_loss: 0.2166 - val_accuracy: 0.9447\n",
      "Epoch 77/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3312 - accuracy: 0.8717 - val_loss: 0.2219 - val_accuracy: 0.9414\n",
      "Epoch 78/100\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.3297 - accuracy: 0.8675 - val_loss: 0.1899 - val_accuracy: 0.9648\n",
      "Epoch 79/100\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.3285 - accuracy: 0.8708 - val_loss: 0.1864 - val_accuracy: 0.9665\n",
      "Epoch 80/100\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.3280 - accuracy: 0.8687 - val_loss: 0.1905 - val_accuracy: 0.9615\n",
      "Epoch 81/100\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.3270 - accuracy: 0.8712 - val_loss: 0.1857 - val_accuracy: 0.9698\n",
      "Epoch 82/100\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3252 - accuracy: 0.8725 - val_loss: 0.1888 - val_accuracy: 0.9682\n",
      "Epoch 83/100\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.3242 - accuracy: 0.8704 - val_loss: 0.1984 - val_accuracy: 0.9548\n",
      "Epoch 84/100\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.3236 - accuracy: 0.8717 - val_loss: 0.1882 - val_accuracy: 0.9715\n",
      "Epoch 85/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3223 - accuracy: 0.8704 - val_loss: 0.1867 - val_accuracy: 0.9682\n",
      "Epoch 86/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3236 - accuracy: 0.8671 - val_loss: 0.2155 - val_accuracy: 0.9447\n",
      "Epoch 87/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3200 - accuracy: 0.8708 - val_loss: 0.1787 - val_accuracy: 0.9765\n",
      "Epoch 88/100\n",
      "75/75 [==============================] - 1s 7ms/step - loss: 0.3203 - accuracy: 0.8721 - val_loss: 0.1755 - val_accuracy: 0.9765\n",
      "Epoch 89/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3202 - accuracy: 0.8704 - val_loss: 0.1970 - val_accuracy: 0.9564\n",
      "Epoch 90/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3182 - accuracy: 0.8712 - val_loss: 0.1865 - val_accuracy: 0.9665\n",
      "Epoch 91/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3168 - accuracy: 0.8708 - val_loss: 0.2321 - val_accuracy: 0.9296\n",
      "Epoch 92/100\n",
      "75/75 [==============================] - 1s 7ms/step - loss: 0.3166 - accuracy: 0.8696 - val_loss: 0.1701 - val_accuracy: 0.9782\n",
      "Epoch 93/100\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.3152 - accuracy: 0.8729 - val_loss: 0.2048 - val_accuracy: 0.9531\n",
      "Epoch 94/100\n",
      "75/75 [==============================] - 0s 7ms/step - loss: 0.3152 - accuracy: 0.8704 - val_loss: 0.1993 - val_accuracy: 0.9531\n",
      "Epoch 95/100\n",
      "75/75 [==============================] - 0s 7ms/step - loss: 0.3139 - accuracy: 0.8712 - val_loss: 0.2439 - val_accuracy: 0.9179\n",
      "Epoch 96/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3148 - accuracy: 0.8700 - val_loss: 0.2211 - val_accuracy: 0.9363\n",
      "Epoch 97/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3120 - accuracy: 0.8712 - val_loss: 0.1940 - val_accuracy: 0.9564\n",
      "Epoch 98/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3125 - accuracy: 0.8725 - val_loss: 0.1921 - val_accuracy: 0.9581\n",
      "Epoch 99/100\n",
      "75/75 [==============================] - 0s 7ms/step - loss: 0.3107 - accuracy: 0.8750 - val_loss: 0.1889 - val_accuracy: 0.9598\n",
      "Epoch 100/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.3085 - accuracy: 0.8758 - val_loss: 0.2118 - val_accuracy: 0.9464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16fe43210>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the function with the created model\n",
    "\n",
    "model = model_neural()\n",
    "\n",
    "model.fit(X_train, y_train, epochs = 100, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Get them into 0 and 1 values\n",
    "\n",
    "binary_values = []\n",
    "\n",
    "# Start a for loop to iterate over the predictions array\n",
    "\n",
    "for i in predictions:\n",
    "    if i < 0.5:\n",
    "        binary_values.append(0)\n",
    "    if i >= 0.5:\n",
    "        binary_values.append(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the confusion matrix to evaluate the model\n",
    "\n",
    "cm = confusion_matrix(y_test, binary_values)\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=['not_flip','flip'])\n",
    "cmd.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95       307\n",
      "           1       0.90      1.00      0.95       290\n",
      "\n",
      "    accuracy                           0.95       597\n",
      "   macro avg       0.95      0.95      0.95       597\n",
      "weighted avg       0.95      0.95      0.95       597\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the Classification report to get the precision, recall, f1-score\n",
    "\n",
    "print(classification_report(y_test, binary_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
